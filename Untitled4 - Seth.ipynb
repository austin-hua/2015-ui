{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/austin-hua/2015-ui/blob/master/Untitled4%20-%20Seth.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8c5QUHNYFLg"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell => Restart the session => Start executing the below cells **(DO NOT EXECUTE THIS CELL AGAIN)**\n",
        "# Core LangChain and AI ecosystem packages\n",
        "!pip install -q \\\n",
        "    langchain==0.3.21 \\\n",
        "    huggingface_hub==0.29.3 \\\n",
        "    openai==1.68.2 \\\n",
        "    chromadb==0.6.3 \\\n",
        "    langchain_openai==0.3.10 \\\n",
        "    langchain-community==0.3.20 \\\n",
        "    lark==1.2.2 \\\n",
        "    rank_bm25==0.2.2 \\\n",
        "    numpy==2.1.0 \\\n",
        "    scipy==1.15.2 \\\n",
        "    scikit-learn==1.6.1 \\\n",
        "    transformers==4.50.0 \\\n",
        "    pypdf==5.4.0 \\\n",
        "    tiktoken==0.9.0 \\\n",
        "    sentence_transformers==4.0.0\n",
        "# Install locally or in notebook:\n",
        "!pip install langchain-community pypdf\n",
        "!pip install -q langchain-community pypdf\n",
        "\n",
        "# Then in Python:\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# PyTorch with CUDA 12.4 support\n",
        "!pip install torch==2.6.0+cu124 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# @title Loading the `config.json` file\n",
        "import json, os\n",
        "\n",
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    os.environ['OPENAI_API_KEY']  = config.get(\"API_KEY\") # Loading the API Key\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\") # Loading the API Base Url\n",
        "\n",
        "# @title Defining the LLM Model - Use `gpt-4o` Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load NOFO and extract topic\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_file = \"/content/NOFO_pdf.pdf\"\n",
        "loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\", mode=\"single\")\n",
        "NOFO_pdf = loader.load() # Changed from pdf_loader.load()\n",
        "doc_text = \"\\n\\n\".join([doc.page_content for doc in NOFO_pdf])\n",
        "doc_text = doc_text[:12000]  # Token-safe truncation\n",
        "\n",
        "\n",
        "# Prompt to extract topic\n",
        "topic_extraction_prompt = f\"\"\"\n",
        "You are a grant analyst. Carefully read the following NOFO text and identify the **topic** for which funding is being provided.\n",
        "\n",
        "Only return the topic name. Do not include any explanation, context, or extra text.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "{doc_text}\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "topic_extraction = llm.invoke(topic_extraction_prompt)\n",
        "topic = topic_extraction.content.strip\n",
        "print(\"Extracted Topic:\", topic)\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip papers\n",
        "with zipfile.ZipFile(\"/content/ResearchPapers.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# Set research paper folder path\n",
        "papers_path = \"/content/Papers/\"\n",
        "\n",
        "\n",
        "# Reload NOFO for use in prompt\n",
        "nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "nofo_docs = nofo_loader.load()\n",
        "NOFO_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "NOFO_text = NOFO_text[:12000]\n",
        "\n",
        "relevance_prompt = f\"\"\"\n",
        "You are an expert grant reviewer and research analyst.\n",
        "\n",
        "Analyze the relevance of the following research paper in relation to the topic, goals, objectives, and funding criteria outlined in the NOFO document.\n",
        "\n",
        "Your task is to:\n",
        "1. Determine whether the research aligns with the funding opportunity goals, objectives, and evaluation priorities.\n",
        "2. Assess whether the research could reasonably be used to support or inspire a viable project proposal under this NOFO.\n",
        "3. Evaluate the relevance based on domain, methodology, or application.\n",
        "\n",
        "If the research paper does not relate to the topic by domain, methodology, or intended application, return:\n",
        "**\"Paper not related to topic\"**\n",
        "\n",
        "### NOFO Topic:\n",
        "{topic}\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "NOFO Document:\n",
        "\\\"\\\"\\\"\n",
        "{NOFO_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Research Paper:\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"  # This ends the prompt template before appending paper text\n",
        "\n",
        "import tiktoken\n",
        "import requests\n",
        "\n",
        "from tiktoken import get_encoding\n",
        "encoding = get_encoding(\"cl100k_base\")\n",
        "MAX_TOKENS = 120000\n",
        "\n",
        "documents = []\n",
        "relevant_papers_count = 0\n",
        "irrelevant_papers_count = 0\n",
        "total_files = len([f for f in os.listdir(papers_path) if f.endswith(\".pdf\")])\n",
        "progress_cnt = 1\n",
        "\n",
        "for filename in os.listdir(papers_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(papers_path, filename)\n",
        "\n",
        "        try:\n",
        "            # Load research paper\n",
        "            paper_loader = PyPDFLoader(file_path,mode=\"single\")\n",
        "            paper_docs = paper_loader.load()\n",
        "            paper_text = paper_docs[0].page_content\n",
        "\n",
        "            # Truncate paper text to fit token limit\n",
        "            base_tokens = len(encoding.encode(relevance_prompt_template))\n",
        "            available_tokens = MAX_TOKENS - base_tokens\n",
        "            truncated_pages = encoding.decode(encoding.encode(paper_text)[:available_tokens])\n",
        "\n",
        "            # Complete the full prompt\n",
        "            full_prompt = relevance_prompt + truncated_pages + '\\n\\\"\\\"\\\"\\n\\nReturn your answer in the following format:\\n- Relevance Summary:\\n- Alignment with NOFO Goals:\\n- Potential for Use in Proposal Development:'\n",
        "\n",
        "            # Invoke LLM\n",
        "            response = llm.invoke(full_prompt)\n",
        "            print(response.content)\n",
        "            print(f\"Successfully processed: {progress_cnt}/{total_files}\")\n",
        "            progress_cnt += 1\n",
        "\n",
        "\t\t\t# if \"PAPER RELEVANT TO TOPIC\" in response.content:\n",
        "\t\t\t#\trelevant_papers_count +1\n",
        "\n",
        "\n",
        "            if \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
        "                irrelevant_papers_count += 1\n",
        "                continue\n",
        "\n",
        "            documents.append({\n",
        "                'title': filename,\n",
        "                'llm_response': response.content,\n",
        "                'file_path': file_path\n",
        "            })\n",
        "            relevant_papers_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {filename}: {str(e)}\")\n",
        "\n",
        "# Summary\n",
        "print(\"=\"*50)\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content[:500])  # First 500 characters\n",
        "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
        "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cibU19LKaJrj",
        "outputId": "29b1420f-93c1-497d-cf45-514ff603e7e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.3/16.3 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.1.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRequirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.6.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'reason': {'error': 'You exceeded your current quota!!'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-9-2882996611.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \"\"\"\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mtopic_extraction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtopic_extraction_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mtopic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopic_extraction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Extracted Topic:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         return cast(\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'reason': {'error': 'You exceeded your current quota!!'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell => Restart the session => Start executing the below cells **(DO NOT EXECUTE THIS CELL AGAIN)**\n",
        "# Core LangChain and AI ecosystem packages\n",
        "!pip install -q \\\n",
        "    langchain==0.3.21 \\\n",
        "    huggingface_hub==0.29.3 \\\n",
        "    openai==1.68.2 \\\n",
        "    chromadb==0.6.3 \\\n",
        "    langchain_openai==0.3.10 \\\n",
        "    langchain-community==0.3.20 \\\n",
        "    lark==1.2.2 \\\n",
        "    rank_bm25==0.2.2 \\\n",
        "    numpy==2.1.0 \\\n",
        "    scipy==1.15.2 \\\n",
        "    scikit-learn==1.6.1 \\\n",
        "    transformers==4.50.0 \\\n",
        "    pypdf==5.4.0 \\\n",
        "    tiktoken==0.9.0 \\\n",
        "    sentence_transformers==4.0.0\n",
        "# Install locally or in notebook:\n",
        "!pip install langchain-community pypdf\n",
        "!pip install -q langchain-community pypdf\n",
        "!pip install tenacity\n",
        "\n",
        "# Then in Python:\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# PyTorch with CUDA 12.4 support\n",
        "!pip install torch==2.6.0+cu124 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# @title Loading the `config.json` file\n",
        "import json, os\n",
        "\n",
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    os.environ['OPENAI_API_KEY']  = config.get(\"API_KEY\") # Loading the API Key\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\") # Loading the API Base Url\n",
        "\n",
        "# @title Defining the LLM Model - Use `gpt-4o` Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load NOFO and extract topic\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_file = \"/content/NOFO_pdf.pdf\"\n",
        "loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\", mode=\"single\")\n",
        "NOFO_pdf = loader.load() # Changed from pdf_loader.load()\n",
        "doc_text = \"\\n\\n\".join([doc.page_content for doc in NOFO_pdf])\n",
        "doc_text = doc_text[:12000]  # Token-safe truncation\n",
        "\n",
        "\n",
        "# Prompt to extract topic\n",
        "topic_extraction_prompt = f\"\"\"\n",
        "You are a grant analyst. Carefully read the following NOFO text and identify the **topic** for which funding is being provided.\n",
        "\n",
        "Only return the topic name. Do not include any explanation, context, or extra text.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "{doc_text}\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# Implement retry mechanism for the LLM call\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(print, logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_llm_with_retry(prompt):\n",
        "    \"\"\"Helper function to invoke LLM with retry logic.\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "# Call the LLM using the retry function\n",
        "try:\n",
        "    topic_extraction = invoke_llm_with_retry(topic_extraction_prompt)\n",
        "    topic = topic_extraction.content.strip() # Added parentheses to call the strip method\n",
        "    print(\"Extracted Topic:\", topic)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to extract topic after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during topic extraction: {e}\")\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip papers\n",
        "with zipfile.ZipFile(\"/content/ResearchPapers.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# Set research paper folder path\n",
        "papers_path = \"/content/Papers/\"\n",
        "\n",
        "\n",
        "# Reload NOFO for use in prompt\n",
        "nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "nofo_docs = nofo_loader.load()\n",
        "NOFO_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "NOFO_text = NOFO_text[:12000]\n",
        "\n",
        "relevance_prompt = f\"\"\"\n",
        "You are an expert grant reviewer and research analyst.\n",
        "\n",
        "Analyze the relevance of the following research paper in relation to the topic, goals, objectives, and funding criteria outlined in the NOFO document.\n",
        "\n",
        "Your task is to:\n",
        "1. Determine whether the research aligns with the funding opportunity goals, objectives, and evaluation priorities.\n",
        "2. Assess whether the research could reasonably be used to support or inspire a viable project proposal under this NOFO.\n",
        "3. Evaluate the relevance based on domain, methodology, or application.\n",
        "\n",
        "If the research paper does not relate to the topic by domain, methodology, or intended application, return:\n",
        "**\"Paper not related to topic\"**\n",
        "\n",
        "### NOFO Topic:\n",
        "{topic}\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "NOFO Document:\n",
        "\\\"\\\"\\\"\n",
        "{NOFO_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Research Paper:\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"  # This ends the prompt template before appending paper text\n",
        "\n",
        "import tiktoken\n",
        "import requests\n",
        "import logging # Import logging for tenacity\n",
        "\n",
        "from tiktoken import get_encoding\n",
        "encoding = get_encoding(\"cl100k_base\")\n",
        "MAX_TOKENS = 120000\n",
        "\n",
        "documents = []\n",
        "relevant_papers_count = 0\n",
        "irrelevant_papers_count = 0\n",
        "total_files = len([f for f in os.listdir(papers_path) if f.endswith(\".pdf\")])\n",
        "progress_cnt = 1\n",
        "\n",
        "for filename in os.listdir(papers_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(papers_path, filename)\n",
        "\n",
        "        try:\n",
        "            # Load research paper\n",
        "            paper_loader = PyPDFLoader(file_path,mode=\"single\")\n",
        "            paper_docs = paper_loader.load()\n",
        "            paper_text = paper_docs[0].page_content\n",
        "\n",
        "            # Truncate paper text to fit token limit\n",
        "            base_tokens = len(encoding.encode(relevance_prompt_template))\n",
        "            available_tokens = MAX_TOKENS - base_tokens\n",
        "            truncated_pages = encoding.decode(encoding.encode(paper_text)[:available_tokens])\n",
        "\n",
        "            # Complete the full prompt\n",
        "            full_prompt = relevance_prompt + truncated_pages + '\\n\\\"\\\"\\\"\\n\\nReturn your answer in the following format:\\n- Relevance Summary:\\n- Alignment with NOFO Goals:\\n- Potential for Use in Proposal Development:'\n",
        "\n",
        "            # Invoke LLM with Tenacity\n",
        "\t\t\t@tenacity.retry(\n",
        "                wait=tenacity.wait_exponential(multiplier=1, min=4, max=60),\n",
        "                stop=tenacity.stop_after_attempt(5),\n",
        "                before_sleep=tenacity.before_sleep_log(print, logging.INFO),\n",
        "                retry=tenacity.retry_if_exception_type(RateLimitError)\n",
        "            )\n",
        "            def invoke_llm_for_relevance(prompt):\n",
        "                \"\"\"Helper function to invoke LLM with retry logic for relevance check.\"\"\"\n",
        "                return llm.invoke(prompt)\n",
        "\n",
        "\t\t\tresponse = llm.invoke(full_prompt)\n",
        "\t\t\tprint(response.content)\n",
        "\n",
        "            print(f\"Successfully processed: {progress_cnt}/{total_files}\")\n",
        "            progress_cnt += 1\n",
        "\n",
        "\t\t\t# if \"PAPER RELEVANT TO TOPIC\" in response.content:\n",
        "\t\t\t#\trelevant_papers_count +1\n",
        "\n",
        "\n",
        "            if \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
        "                irrelevant_papers_count += 1\n",
        "                continue\n",
        "\n",
        "            documents.append({\n",
        "                'title': filename,\n",
        "                'llm_response': response.content,\n",
        "                'file_path': file_path\n",
        "            })\n",
        "            relevant_papers_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {filename}: {str(e)}\")\n",
        "\n",
        "# Summary\n",
        "print(\"=\"*50)\n",
        "# Make sure 'loader' is still the NOFO loader here\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content[:500])  # First 500 characters\n",
        "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
        "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "_guwkAqehlID",
        "outputId": "039cd231-6467-4d16-ce08-4701bba49c65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TabError",
          "evalue": "inconsistent use of tabs and spaces in indentation (ipython-input-10-733272735.py, line 159)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-10-733272735.py\"\u001b[0;36m, line \u001b[0;32m159\u001b[0m\n\u001b[0;31m    @tenacity.retry(\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mTabError\u001b[0m\u001b[0;31m:\u001b[0m inconsistent use of tabs and spaces in indentation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell => Restart the session => Start executing the below cells **(DO NOT EXECUTE THIS CELL AGAIN)**\n",
        "# Core LangChain and AI ecosystem packages\n",
        "!pip install -q \\\n",
        "    langchain==0.3.21 \\\n",
        "    huggingface_hub==0.29.3 \\\n",
        "    openai==1.68.2 \\\n",
        "    chromadb==0.6.3 \\\n",
        "    langchain_openai==0.3.10 \\\n",
        "    langchain-community==0.3.20 \\\n",
        "    lark==1.2.2 \\\n",
        "    rank_bm25==0.2.2 \\\n",
        "    numpy==2.1.0 \\\n",
        "    scipy==1.15.2 \\\n",
        "    scikit-learn==1.6.1 \\\n",
        "    transformers==4.50.0 \\\n",
        "    pypdf==5.4.0 \\\n",
        "    tiktoken==0.9.0 \\\n",
        "    sentence_transformers==4.0.0\n",
        "# Install locally or in notebook:\n",
        "!pip install langchain-community pypdf\n",
        "!pip install -q langchain-community pypdf\n",
        "!pip install tenacity\n",
        "\n",
        "# Then in Python:\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# PyTorch with CUDA 12.4 support\n",
        "!pip install torch==2.6.0+cu124 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# @title Loading the `config.json` file\n",
        "import json, os\n",
        "\n",
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    os.environ['OPENAI_API_KEY']  = config.get(\"API_KEY\") # Loading the API Key\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\") # Loading the API Base Url\n",
        "\n",
        "# @title Defining the LLM Model - Use `gpt-4o` Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load NOFO and extract topic\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_file = \"/content/NOFO_pdf.pdf\"\n",
        "loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\", mode=\"single\")\n",
        "NOFO_pdf = loader.load() # Changed from pdf_loader.load()\n",
        "doc_text = \"\\n\\n\".join([doc.page_content for doc in NOFO_pdf])\n",
        "doc_text = doc_text[:12000]  # Token-safe truncation\n",
        "\n",
        "\n",
        "# Prompt to extract topic\n",
        "topic_extraction_prompt = f\"\"\"\n",
        "You are a grant analyst. Carefully read the following NOFO text and identify the **topic** for which funding is being provided.\n",
        "\n",
        "Only return the topic name. Do not include any explanation, context, or extra text.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "{doc_text}\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# Initialize topic to a default value before the try block\n",
        "topic = \"\"\n",
        "\n",
        "# Implement retry mechanism for the LLM call\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_llm_with_retry(prompt):\n",
        "    \"\"\"Helper function to invoke LLM with retry logic.\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "# Call the LLM using the retry function\n",
        "try:\n",
        "    topic_extraction = invoke_llm_with_retry(topic_extraction_prompt)\n",
        "    topic = topic_extraction.content.strip() # Added parentheses to call the strip method\n",
        "    print(\"Extracted Topic:\", topic)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to extract topic after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during topic extraction: {e}\")\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip papers\n",
        "with zipfile.ZipFile(\"/content/ResearchPapers.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# Set research paper folder path\n",
        "papers_path = \"/content/Papers/\"\n",
        "\n",
        "\n",
        "# Reload NOFO for use in prompt\n",
        "nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "nofo_docs = nofo_loader.load()\n",
        "NOFO_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "NOFO_text = NOFO_text[:12000]\n",
        "\n",
        "relevance_prompt_template = f\"\"\"\n",
        "You are an expert grant reviewer and research analyst.\n",
        "\n",
        "Analyze the relevance of the following research paper in relation to the topic, goals, objectives, and funding criteria outlined in the NOFO document.\n",
        "\n",
        "Your task is to:\n",
        "1. Determine whether the research aligns with the funding opportunity goals, objectives, and evaluation priorities.\n",
        "2. Assess whether the research could reasonably be used to support or inspire a viable project proposal under this NOFO.\n",
        "3. Evaluate the relevance based on domain, methodology, or application.\n",
        "\n",
        "If the research paper does not relate to the topic by domain, methodology, or intended application, return:\n",
        "**\"Paper not related to topic\"**\n",
        "\n",
        "### NOFO Topic:\n",
        "{topic}\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "NOFO Document:\n",
        "\\\"\\\"\\\"\n",
        "{NOFO_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Research Paper:\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"  # This ends the prompt template before appending paper text\n",
        "\n",
        "# Define the helper function for invoking LLM for relevance check outside the loop\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60),\n",
        "    stop=tenacity.stop_after_attempt(5),\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO),\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError)\n",
        ")\n",
        "def invoke_llm_for_relevance(prompt):\n",
        "    \"\"\"Helper function to invoke LLM with retry logic for relevance check.\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "import tiktoken\n",
        "import requests\n",
        "import logging # Import logging for tenacity\n",
        "\n",
        "from tiktoken import get_encoding\n",
        "encoding = get_encoding(\"cl100k_base\")\n",
        "MAX_TOKENS = 120000\n",
        "\n",
        "documents = []\n",
        "relevant_papers_count = 0\n",
        "irrelevant_papers_count = 0\n",
        "total_files = len([f for f in os.listdir(papers_path) if f.endswith(\".pdf\")])\n",
        "progress_cnt = 1\n",
        "\n",
        "for filename in os.listdir(papers_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(papers_path, filename)\n",
        "\n",
        "        try:\n",
        "            # Load research paper\n",
        "            paper_loader = PyPDFLoader(file_path,mode=\"single\")\n",
        "            paper_docs = paper_loader.load()\n",
        "            paper_text = paper_docs[0].page_content\n",
        "\n",
        "            # Truncate paper text to fit token limit\n",
        "            base_tokens = len(encoding.encode(relevance_prompt_template))\n",
        "            available_tokens = MAX_TOKENS - base_tokens\n",
        "            truncated_pages = encoding.decode(encoding.encode(paper_text)[:available_tokens])\n",
        "\n",
        "            # Complete the full prompt\n",
        "            full_prompt = relevance_prompt_template + truncated_pages + '\\n\\\"\\\"\\\"\\n\\nReturn your answer in the following format:\\n- Relevance Summary:\\n- Alignment with NOFO Goals:\\n- Potential for Use in Proposal Development:'\n",
        "\n",
        "            # Invoke LLM using the helper function\n",
        "            response = invoke_llm_for_relevance(full_prompt)\n",
        "            print(response.content)\n",
        "\n",
        "            print(f\"Successfully processed: {progress_cnt}/{total_files}\")\n",
        "            progress_cnt += 1\n",
        "\n",
        "            # if \"PAPER RELEVANT TO TOPIC\" in response.content:\n",
        "            #   relevant_papers_count +1\n",
        "\n",
        "\n",
        "            if \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
        "                irrelevant_papers_count += 1\n",
        "                continue\n",
        "\n",
        "            documents.append({\n",
        "                'title': filename,\n",
        "                'llm_response': response.content,\n",
        "                'file_path': file_path\n",
        "            })\n",
        "            relevant_papers_count += 1\n",
        "\n",
        "        except RateLimitError as e:\n",
        "             print(f\"!!! Rate limit error processing {filename}: {str(e)}. Skipping for now.\")\n",
        "             irrelevant_papers_count += 1 # Or handle skipped files differently\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {filename}: {str(e)}\")\n",
        "            irrelevant_papers_count += 1 # Consider papers with processing errors as irrelevant for the count\n",
        "\n",
        "# Summary\n",
        "print(\"=\"*50)\n",
        "# Make sure 'loader' is still the NOFO loader here\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content[:500])  # First 500 characters\n",
        "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
        "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "5HnOte35j2P7",
        "outputId": "4fa988a7-6e2b-48ce-c29a-9bc2224a9653"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.6.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An unexpected error occurred during topic extraction: RetryError[<Future at 0x7a9717595910 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Vol33Iss1_INSNApdf.pdf: RetryError[<Future at 0x7a971b9f1b10 state=finished raised RateLimitError>]\n",
            "!!! Error processing RES2D.pdf: RetryError[<Future at 0x7a97286369d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Sailer McCulloh Soc Net and Spatial Config.pdf: RetryError[<Future at 0x7a971b9d6910 state=finished raised RateLimitError>]\n",
            "!!! Error processing McCullohCarleyJOSS.pdf: RetryError[<Future at 0x7a971b9ddfd0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots.pdf: RetryError[<Future at 0x7a971de07650 state=finished raised RateLimitError>]\n",
            "!!! Error processing jfq-110_46-53_Cruickshank.pdf: RetryError[<Future at 0x7a9728635dd0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Multi_Agent_Systems_for_Frame_Detection.pdf: RetryError[<Future at 0x7a971b93d7d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing BotBuster___AAAI.pdf: RetryError[<Future at 0x7a97233a08d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Spectral Analysis SNA.pdf: RetryError[<Future at 0x7a97117c3510 state=finished raised RateLimitError>]\n",
            "!!! Error processing Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic.pdf: RetryError[<Future at 0x7a9723268d90 state=finished raised RateLimitError>]\n",
            "!!! Error processing NBA Performance.pdf: RetryError[<Future at 0x7a972861f650 state=finished raised RateLimitError>]\n",
            "!!! Error processing docnet.pdf: RetryError[<Future at 0x7a971bba0f50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Take_boards.pdf: RetryError[<Future at 0x7a972337c890 state=finished raised RateLimitError>]\n",
            "!!! Error processing Frontiers COVID.pdf: RetryError[<Future at 0x7a97235643d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Utility Seeking in Complex Social Systems.pdf: RetryError[<Future at 0x7a9723367a10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing ONA-in-R.pdf: RetryError[<Future at 0x7a97221eb950 state=finished raised RateLimitError>]\n",
            "!!! Error processing ONA-using-igraph.pdf: RetryError[<Future at 0x7a9723240b90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Quantifying_Information_Advantage.pdf: RetryError[<Future at 0x7a970e50eb90 state=finished raised RateLimitError>]\n",
            "!!! Error processing EmergencyResponseAI.pdf: RetryError[<Future at 0x7a97233340d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse.pdf: RetryError[<Future at 0x7a972106db10 state=finished raised RateLimitError>]\n",
            "!!! Error processing The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks.pdf: RetryError[<Future at 0x7a9723357e90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Arrow White Paper DExTra.pdf: RetryError[<Future at 0x7a97284cb650 state=finished raised RateLimitError>]\n",
            "!!! Error processing ALL18.pdf: RetryError[<Future at 0x7a9723374d10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Reforming Sectarian Beliefs.pdf: RetryError[<Future at 0x7a972323d0d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing LongNetViewerORA.pdf: RetryError[<Future at 0x7a972337d210 state=finished raised RateLimitError>]\n",
            "!!! Error processing Chat GPT Bias final w copyright.pdf: RetryError[<Future at 0x7a971b9a7090 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 48 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 92 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing LLM_UQ.pdf: RetryError[<Future at 0x7a971b9d60d0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x8fa1a for key /PageMode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing cycon-final-draft.pdf: RetryError[<Future at 0x7a9722155590 state=finished raised RateLimitError>]\n",
            "!!! Error processing HIV.pdf: RetryError[<Future at 0x7a972241ccd0 state=finished raised RateLimitError>]\n",
            "!!! Error processing YouTube-COVID.pdf: RetryError[<Future at 0x7a972234e810 state=finished raised RateLimitError>]\n",
            "!!! Error processing Tweets-to-touchdowns.pdf: RetryError[<Future at 0x7a9723331f50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Helene_and_Milton_ACM.pdf: RetryError[<Future at 0x7a971b9dcd50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Kent2022_Chapter_MicroscopicMarkovChainApproach.pdf: RetryError[<Future at 0x7a97286702d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Leadership of Data Annotation 20180304v2.pdf: RetryError[<Future at 0x7a9723307350 state=finished raised RateLimitError>]\n",
            "!!! Error processing IkeNet.pdf: RetryError[<Future at 0x7a971ba0a710 state=finished raised RateLimitError>]\n",
            "!!! Error processing Course Info Security.pdf: RetryError[<Future at 0x7a971cccaed0 state=finished raised RateLimitError>]\n",
            "!!! Error processing IkekNet1.pdf: RetryError[<Future at 0x7a9723235450 state=finished raised RateLimitError>]\n",
            "!!! Error processing Social_Det_COVID_Mortality.pdf: RetryError[<Future at 0x7a97224f5590 state=finished raised RateLimitError>]\n",
            "!!! Error processing NeuroSynchrony.pdf: RetryError[<Future at 0x7a971b98b410 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 11 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 58 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 71 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 73 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 75 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 88 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 96 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 98 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 141 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 143 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 150 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 156 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 191 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 200 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 206 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 249 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 261 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 271 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 323 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 325 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 345 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 352 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 397 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 400 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 402 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 416 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 444 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 473 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 475 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 506 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 512 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 522 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 710 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 712 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 718 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 724 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 730 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 900 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 902 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 904 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 942 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 954 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1094 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1146 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1148 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1150 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1156 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1179 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1271 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1273 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1292 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1302 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1321 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1356 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1358 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1368 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1370 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1425 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1427 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1429 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1452 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1458 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1460 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1462 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1497 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1538 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1544 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1575 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1577 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1592 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1598 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1625 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1627 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1629 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1639 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1645 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5.pdf: RetryError[<Future at 0x7a971ab88d10 state=finished raised RateLimitError>]\n",
            "!!! Error processing NAP Behavioral Sci Intel.pdf: RetryError[<Future at 0x7a97286f8310 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Encyclopedia of SNA - R Packages.pdf: RetryError[<Future at 0x7a97119a4c10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 35 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 37 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 45 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 53 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 55 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 57 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 59 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 61 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing FSS-19_paper_137.pdf: RetryError[<Future at 0x7a97221eb8d0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 36 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 40 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 42 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 52 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 54 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing k-truss.pdf: RetryError[<Future at 0x7a971cc65850 state=finished raised RateLimitError>]\n",
            "!!! Error processing Food Addiction 20231222 v3.pdf: RetryError[<Future at 0x7a97286b25d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing LLM_Confidence_Metrics.pdf: RetryError[<Future at 0x7a97230339d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Kidney_Behavioral.pdf: RetryError[<Future at 0x7a972867e350 state=finished raised RateLimitError>]\n",
            "!!! Error processing improving-decision-support-for-organ-transplant.pdf: RetryError[<Future at 0x7a9728665b10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Parler_Disinformation_Challenge___CMOT_Extended.pdf: RetryError[<Future at 0x7a970c8c5c10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Political Party Cohesion.pdf: RetryError[<Future at 0x7a97284b45d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing WEIRD.pdf: RetryError[<Future at 0x7a972867dc50 state=finished raised RateLimitError>]\n",
            "!!! Error processing A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities.pdf: RetryError[<Future at 0x7a9710f4a190 state=finished raised RateLimitError>]\n",
            "!!! Error processing TrainingSetSize.pdf: RetryError[<Future at 0x7a9722375510 state=finished raised RateLimitError>]\n",
            "!!! Error processing Multi_view_Clustering_for_Social_Based_Data.pdf: RetryError[<Future at 0x7a9722f45890 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://aibe.mygreatlearning.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1001\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# thrown on 4xx and 5xx status code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    828\u001b[0m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merror_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merror_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 829\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mHTTPStatusError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPStatusError\u001b[0m: Client error '429 Too Many Requests' for url 'https://aibe.mygreatlearning.com/openai/v1/chat/completions'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/429",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-15-936459927.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m             \u001b[0;31m# Invoke LLM using the helper function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minvoke_llm_for_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36mwrapped_f\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    336\u001b[0m             \u001b[0mcopy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m             \u001b[0mwrapped_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatistics\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mretry_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mWrappedFn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0mretry_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRetryCallState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_object\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mdo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36miter\u001b[0;34m(self, retry_state)\u001b[0m\n\u001b[1;32m    376\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0maction\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 378\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    379\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    380\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(rs)\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_post_retry_check_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretry_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"RetryCallState\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_explicit_retry\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretry_run_result\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_action_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutcome\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    401\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tenacity/__init__.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m    478\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDoAttempt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 480\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    481\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: B902\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    482\u001b[0m                     \u001b[0mretry_state\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-15-936459927.py\u001b[0m in \u001b[0;36minvoke_llm_for_relevance\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minvoke_llm_for_relevance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \u001b[0;34m\"\"\"Helper function to invoke LLM with retry logic for relevance check.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtiktoken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    370\u001b[0m         return cast(\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    374\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_openai/chat_models/base.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    923\u001b[0m             \u001b[0mgeneration_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"headers\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    924\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 925\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mpayload\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    926\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    927\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0;31m# In a synchronous context we are blocking the entire thread. Up to the library user to run the client in a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m         \u001b[0;31m# different thread if necessary.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1055\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1057\u001b[0m         return self._request(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Run this cell => Restart the session => Start executing the below cells **(DO NOT EXECUTE THIS CELL AGAIN)**\n",
        "# Core LangChain and AI ecosystem packages\n",
        "!pip install -q \\\n",
        "    langchain==0.3.21 \\\n",
        "    huggingface_hub==0.29.3 \\\n",
        "    openai==1.68.2 \\\n",
        "    chromadb==0.6.3 \\\n",
        "    langchain_openai==0.3.10 \\\n",
        "    langchain-community==0.3.20 \\\n",
        "    lark==1.2.2 \\\n",
        "    rank_bm25==0.2.2 \\\n",
        "    numpy==2.1.0 \\\n",
        "    numpy==2.1 \\\n",
        "    scipy==1.15.2 \\\n",
        "    scikit-learn==1.6.1 \\\n",
        "    transformers==4.50.0 \\\n",
        "    pypdf==5.4.0 \\\n",
        "    tiktoken==0.9.0 \\\n",
        "    sentence_transformers==4.0.0\n",
        "# Install locally or in notebook:\n",
        "!pip install langchain-community pypdf\n",
        "!pip install -q langchain-community pypdf\n",
        "!pip install tenacity\n",
        "\n",
        "# Then in Python:\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "# PyTorch with CUDA 12.4 support\n",
        "!pip install torch==2.6.0+cu124 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# @title Loading the `config.json` file\n",
        "import json, os\n",
        "\n",
        "# Load the JSON file and extract values\n",
        "file_name = 'config.json'\n",
        "with open(file_name, 'r') as file:\n",
        "    config = json.load(file)\n",
        "    os.environ['OPENAI_API_KEY']  = config.get(\"API_KEY\") # Loading the API Key\n",
        "    os.environ[\"OPENAI_BASE_URL\"] = config.get(\"OPENAI_API_BASE\") # Loading the API Base Url\n",
        "\n",
        "# @title Defining the LLM Model - Use `gpt-4o` Model\n",
        "from langchain_openai import ChatOpenAI\n",
        "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
        "\n",
        "# from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load NOFO and extract topic\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "pdf_file = \"/content/NOFO_pdf.pdf\"\n",
        "loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\", mode=\"single\")\n",
        "NOFO_pdf = loader.load() # Changed from pdf_loader.load()\n",
        "doc_text = \"\\n\\n\".join([doc.page_content for doc in NOFO_pdf])\n",
        "doc_text = doc_text[:12000]  # Token-safe truncation\n",
        "\n",
        "# Prompt to extract topic\n",
        "topic_extraction_prompt = f\"\"\"\n",
        "You are a grant analyst. Carefully read the following NOFO text and identify the **topic** for which funding is being provided.\n",
        "\n",
        "Only return the topic name. Do not include any explanation, context, or extra text.\n",
        "\n",
        "Text:\n",
        "\\\"\\\"\\\"\n",
        "{doc_text}\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"\n",
        "\n",
        "# Initialize topic to a default value before the try block\n",
        "topic = \"\"\n",
        "\n",
        "# Implement retry mechanism for the LLM call\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_llm_with_retry(prompt):\n",
        "    \"\"\"Helper function to invoke LLM with retry logic.\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "# Call the LLM using the retry function\n",
        "try:\n",
        "    topic_extraction = invoke_llm_with_retry(topic_extraction_prompt)\n",
        "    topic = topic_extraction.content.strip() # Added parentheses to call the strip method\n",
        "    print(\"Extracted Topic:\", topic)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to extract topic after multiple retries due to rate limit: {e}\")\n",
        "    # Consider assigning a placeholder or handling this case explicitly\n",
        "    topic = \"Unknown Topic (Rate Limit Error)\" # Assign a default or error message\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during topic extraction: {e}\")\n",
        "    # Consider assigning a placeholder or handling this case explicitly\n",
        "    topic = \"Unknown Topic (Processing Error)\" # Assign a default or error message\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Unzip papers\n",
        "with zipfile.ZipFile(\"/content/ResearchPapers.zip\", 'r') as zip_ref:\n",
        "    zip_ref.extractall(\"/content/\")\n",
        "\n",
        "# Set research paper folder path\n",
        "papers_path = \"/content/Papers/\"\n",
        "\n",
        "\n",
        "# Reload NOFO for use in prompt\n",
        "nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "nofo_docs = nofo_loader.load()\n",
        "NOFO_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "NOFO_text = NOFO_text[:12000]\n",
        "\n",
        "relevance_prompt_template = f\"\"\"\n",
        "You are an expert grant reviewer and research analyst.\n",
        "\n",
        "Analyze the relevance of the following research paper in relation to the topic, goals, objectives, and funding criteria outlined in the NOFO document.\n",
        "\n",
        "Your task is to:\n",
        "1. Determine whether the research aligns with the funding opportunity goals, objectives, and evaluation priorities.\n",
        "2. Assess whether the research could reasonably be used to support or inspire a viable project proposal under this NOFO.\n",
        "3. Evaluate the relevance based on domain, methodology, or application.\n",
        "\n",
        "If the research paper does not relate to the topic by domain, methodology, or intended application, return:\n",
        "**\"Paper not related to topic\"**\n",
        "\n",
        "### NOFO Topic:\n",
        "{topic}\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "NOFO Document:\n",
        "\\\"\\\"\\\"\n",
        "{NOFO_text}\n",
        "\\\"\\\"\\\"\n",
        "\n",
        "Research Paper:\n",
        "\\\"\\\"\\\"\n",
        "\"\"\"  # This ends the prompt template before appending paper text\n",
        "\n",
        "# Define the helper function for invoking LLM for relevance check outside the loop\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60),\n",
        "    stop=tenacity.stop_after_attempt(5),\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO),\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError)\n",
        ")\n",
        "def invoke_llm_for_relevance(prompt):\n",
        "    \"\"\"Helper function to invoke LLM with retry logic for relevance check.\"\"\"\n",
        "    return llm.invoke(prompt)\n",
        "\n",
        "import tiktoken\n",
        "import requests\n",
        "import logging # Import logging for tenacity\n",
        "\n",
        "from tiktoken import get_encoding\n",
        "encoding = get_encoding(\"cl100k_base\")\n",
        "MAX_TOKENS = 120000\n",
        "\n",
        "documents = []\n",
        "relevant_papers_count = 0\n",
        "irrelevant_papers_count = 0\n",
        "total_files = len([f for f in os.listdir(papers_path) if f.endswith(\".pdf\")])\n",
        "progress_cnt = 1\n",
        "\n",
        "for filename in os.listdir(papers_path):\n",
        "    if filename.endswith(\".pdf\"):\n",
        "        file_path = os.path.join(papers_path, filename)\n",
        "\n",
        "        try:\n",
        "            # Load research paper\n",
        "            paper_loader = PyPDFLoader(file_path,mode=\"single\")\n",
        "            paper_docs = paper_loader.load()\n",
        "            paper_text = paper_docs[0].page_content\n",
        "\n",
        "            # Truncate paper text to fit token limit\n",
        "            base_tokens = len(encoding.encode(relevance_prompt_template))\n",
        "            available_tokens = MAX_TOKENS - base_tokens\n",
        "            truncated_pages = encoding.decode(encoding.encode(paper_text)[:available_tokens])\n",
        "\n",
        "            # Complete the full prompt\n",
        "            full_prompt = relevance_prompt_template + truncated_pages + '\\n\\\"\\\"\\\"\\n\\nReturn your answer in the following format:\\n- Relevance Summary:\\n- Alignment with NOFO Goals:\\n- Potential for Use in Proposal Development:'\n",
        "\n",
        "            # Invoke LLM using the helper function\n",
        "            response = invoke_llm_for_relevance(full_prompt)\n",
        "            # print(response.content)\n",
        "\n",
        "            print(f\"Successfully processed: {progress_cnt}/{total_files}\")\n",
        "            progress_cnt += 1\n",
        "\n",
        "            # if \"PAPER RELEVANT TO TOPIC\" in response.content:\n",
        "            #   relevant_papers_count +1\n",
        "\n",
        "\n",
        "            if \"PAPER NOT RELATED TO TOPIC\" in response.content:\n",
        "                irrelevant_papers_count += 1\n",
        "                continue\n",
        "\n",
        "            documents.append({\n",
        "                'title': filename,\n",
        "                'llm_response': response.content,\n",
        "                'file_path': file_path\n",
        "            })\n",
        "            relevant_papers_count += 1\n",
        "\n",
        "        except RateLimitError as e:\n",
        "             print(f\"!!! Rate limit error processing {filename}: {str(e)}. Skipping for now.\")\n",
        "             irrelevant_papers_count += 1 # Or handle skipped files differently\n",
        "        except Exception as e:\n",
        "            print(f\"!!! Error processing {filename}: {str(e)}\")\n",
        "            irrelevant_papers_count += 1 # Consider papers with processing errors as irrelevant for the count\n",
        "\n",
        "# Summary\n",
        "print(\"=\"*50)\n",
        "# Make sure 'loader' is still the NOFO loader here\n",
        "docs = loader.load()\n",
        "print(docs[0].page_content[:500])  # First 500 characters\n",
        "print(f\"Relevant Papers: {relevant_papers_count}/{total_files}\")\n",
        "print(f\"Irrelevant Papers: {irrelevant_papers_count}/{total_files}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5E4B02PYLaMT",
        "outputId": "af8d0393-133b-421e-9075-4ce988f0af69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.20)\n",
            "Requirement already satisfied: pypdf in /usr/local/lib/python3.11/dist-packages (5.4.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.65)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.9.1)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.1.0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Requirement already satisfied: tenacity in /usr/local/lib/python3.11/dist-packages (9.1.2)\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu124\n",
            "Requirement already satisfied: torch==2.6.0+cu124 in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0+cu124) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0+cu124) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0+cu124) (3.0.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "An unexpected error occurred during topic extraction: RetryError[<Future at 0x7b5bac128d90 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Multi_view_Clustering_for_Social_Based_Data.pdf: RetryError[<Future at 0x7b5bac047b90 state=finished raised RateLimitError>]\n",
            "!!! Error processing jfq-110_46-53_Cruickshank.pdf: RetryError[<Future at 0x7b5bad653f10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing BotBuster___AAAI.pdf: RetryError[<Future at 0x7b5bac9ac050 state=finished raised RateLimitError>]\n",
            "!!! Error processing Political Party Cohesion.pdf: RetryError[<Future at 0x7b5bad354cd0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 28 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Encyclopedia of SNA - R Packages.pdf: RetryError[<Future at 0x7b5baa683990 state=finished raised RateLimitError>]\n",
            "!!! Error processing Extreme Cohesion Darknet 20190815.pdf: RetryError[<Future at 0x7b5bad39f590 state=finished raised RateLimitError>]\n",
            "!!! Error processing Sim of Decon.pdf: RetryError[<Future at 0x7b5bac314b50 state=finished raised RateLimitError>]\n",
            "!!! Error processing DIVERSE_LLM_Dataset___IEEE_Big_Data.pdf: RetryError[<Future at 0x7b5bad9a0f50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Cohort_Optimization_Methods_SNAMS_2021_working_draft (4).pdf: RetryError[<Future at 0x7b5babfccb50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Political_Networks_Conference.pdf: RetryError[<Future at 0x7b5bac3fa990 state=finished raised RateLimitError>]\n",
            "!!! Error processing ICWSM_2025_Political_Bias.pdf: RetryError[<Future at 0x7b5bae00f090 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x8fa1a for key /PageMode\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Take_boards.pdf: RetryError[<Future at 0x7b5bacd3aa10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 9 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 51 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 54 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 56 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing HIV.pdf: RetryError[<Future at 0x7b5baca4b650 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 40 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing AAAI IAA CV.pdf: RetryError[<Future at 0x7b5bad9a22d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Knowing the Terrain.pdf: RetryError[<Future at 0x7b5bad6daa90 state=finished raised RateLimitError>]\n",
            "!!! Error processing NBA Performance.pdf: RetryError[<Future at 0x7b5bacd39e10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 28 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing WEIRD.pdf: RetryError[<Future at 0x7b5bac005550 state=finished raised RateLimitError>]\n",
            "!!! Error processing MOOC 20190828.pdf: RetryError[<Future at 0x7b5bacec6c90 state=finished raised RateLimitError>]\n",
            "!!! Error processing The ABCs of AI-Enabled Intelligence Analysis - War on the Rocks.pdf: RetryError[<Future at 0x7b5bac1288d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Leveraging_AI_to_Improve_Viral_Information_Detection_in_Online_Discourse.pdf: RetryError[<Future at 0x7b5bac047090 state=finished raised RateLimitError>]\n",
            "!!! Error processing Benson_MA491_NLP.pdf: RetryError[<Future at 0x7b5bad305410 state=finished raised RateLimitError>]\n",
            "!!! Error processing ONA-in-R.pdf: RetryError[<Future at 0x7b5bac3f8790 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Social Media Mental Health Final.pdf: RetryError[<Future at 0x7b5bac1f05d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Dormant Bots 20190814.pdf: RetryError[<Future at 0x7b5bad39f6d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing CausalOrgInorgContent.pdf: RetryError[<Future at 0x7b5babfcef90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Dissertation.pdf: RetryError[<Future at 0x7b5bad0bb8d0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 35 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 37 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 45 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 53 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 55 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 57 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 59 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 61 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing RES2D.pdf: RetryError[<Future at 0x7b5bad3f93d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing k-truss.pdf: RetryError[<Future at 0x7b5bac124e90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Multi_Agent_Systems_for_Frame_Detection.pdf: RetryError[<Future at 0x7b5bace38b10 state=finished raised RateLimitError>]\n",
            "!!! Error processing McCullohCarleyJOSS.pdf: RetryError[<Future at 0x7b5bacb78850 state=finished raised RateLimitError>]\n",
            "!!! Error processing Limit Velocity.pdf: RetryError[<Future at 0x7b5bac1a3b90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Hashtag_Revival.pdf: RetryError[<Future at 0x7b5bad3cd010 state=finished raised RateLimitError>]\n",
            "!!! Error processing Kidney_Behavioral.pdf: RetryError[<Future at 0x7b5bad350dd0 state=finished raised RateLimitError>]\n",
            "!!! Error processing SocNetChgDet.pdf: RetryError[<Future at 0x7b5bacb36d90 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 68 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing 2021_EPJ_MVMCInfoOps.pdf: RetryError[<Future at 0x7b5bac374b50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Review of R Packages_20161026.pdf: RetryError[<Future at 0x7b5bac94ced0 state=finished raised RateLimitError>]\n",
            "!!! Error processing TrainingSetSize.pdf: RetryError[<Future at 0x7b5bac3744d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing 23-US-DHS-001.pdf: RetryError[<Future at 0x7b5bac9ada90 state=finished raised RateLimitError>]\n",
            "!!! Error processing On the Science of Networks.pdf: RetryError[<Future at 0x7b5bac33ef90 state=finished raised RateLimitError>]\n",
            "!!! Error processing NeuroSynchrony.pdf: RetryError[<Future at 0x7b5bac155410 state=finished raised RateLimitError>]\n",
            "!!! Error processing Clustering_Analysis_of_Website_Usage_on_Twitter_during_the_COVID_19_Pandemic.pdf: RetryError[<Future at 0x7b5bacb76f90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Parler_Disinformation_Challenge___CMOT_Extended.pdf: RetryError[<Future at 0x7b5bac1eaa10 state=finished raised RateLimitError>]\n",
            "!!! Error processing White Paper Brain Gaze.pdf: RetryError[<Future at 0x7b5bac150450 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Acquiring Maintainable AI_Enable Systems_Final.pdf: RetryError[<Future at 0x7b5bac658090 state=finished raised RateLimitError>]\n",
            "!!! Error processing SM Customer Feedback_FAB_2019_rev3.pdf: RetryError[<Future at 0x7b5bad37f450 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 15 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 19 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 30 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 32 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 44 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Sailer McCulloh Soc Net and Spatial Config.pdf: RetryError[<Future at 0x7b5bacb53c90 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 28 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing NeuroCogInfluence.pdf: RetryError[<Future at 0x7b5bac3e8c50 state=finished raised RateLimitError>]\n",
            "!!! Error processing COVID Bayesian Data Aug.pdf: RetryError[<Future at 0x7b5bac94d810 state=finished raised RateLimitError>]\n",
            "!!! Error processing Utility Seeking in Complex Social Systems.pdf: RetryError[<Future at 0x7b5bacd38710 state=finished raised RateLimitError>]\n",
            "!!! Error processing Characterizing_Communities_of_Hashtag_Usage_on_Twitter_During_the_2020_COVID_19_Pandemic.pdf: RetryError[<Future at 0x7b5baa7e23d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Quantifying_Information_Advantage.pdf: RetryError[<Future at 0x7b5bacb01d50 state=finished raised RateLimitError>]\n",
            "!!! Error processing Confidence_Chaining.pdf: RetryError[<Future at 0x7b5bac33e110 state=finished raised RateLimitError>]\n",
            "!!! Error processing Text Analysis Using Automated Language Translators.pdf: RetryError[<Future at 0x7b5bad30fc90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Cross_Platform_Information_Spread_During_the_January_6th_Capitol_Riots.pdf: RetryError[<Future at 0x7b5bac144550 state=finished raised RateLimitError>]\n",
            "!!! Error processing Frontiers COVID.pdf: RetryError[<Future at 0x7b5bace42810 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 15 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Kent2022_Chapter_MicroscopicMarkovChainApproach.pdf: RetryError[<Future at 0x7b5bac1aefd0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 48 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 92 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Symbolic Generative AI 20231012.pdf: RetryError[<Future at 0x7b5bacec41d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing cycon-final-draft.pdf: RetryError[<Future at 0x7b5bac65b790 state=finished raised RateLimitError>]\n",
            "!!! Error processing Helene_and_Milton_ACM.pdf: RetryError[<Future at 0x7b5bad3e34d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing LSA email.pdf: RetryError[<Future at 0x7b5bad338e90 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 36 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 40 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 42 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 52 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 54 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Lessons from Advising in Afghanistan.pdf: RetryError[<Future at 0x7b5bac1a08d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Food Addiction 20231222 v3.pdf: RetryError[<Future at 0x7b5bac94e9d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Social_Det_COVID_Mortality.pdf: RetryError[<Future at 0x7b5bacb19650 state=finished raised RateLimitError>]\n",
            "!!! Error processing Chat GPT Bias final w copyright.pdf: RetryError[<Future at 0x7b5bac121a10 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 50 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing IkekNet1.pdf: RetryError[<Future at 0x7b5bac39ab10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Designed Networks.pdf: RetryError[<Future at 0x7b5bad307410 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 21 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 23 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 29 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 52 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing LongNetViewerORA.pdf: RetryError[<Future at 0x7b5bad32aa10 state=finished raised RateLimitError>]\n",
            "!!! Error processing MIPB-CDA.pdf: RetryError[<Future at 0x7b5bac33fa90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Leadership of Data Annotation 20180304v2.pdf: RetryError[<Future at 0x7b5bad39ef90 state=finished raised RateLimitError>]\n",
            "!!! Error processing Tweets-to-touchdowns.pdf: RetryError[<Future at 0x7b5bace40450 state=finished raised RateLimitError>]\n",
            "!!! Error processing Spectral Analysis SNA.pdf: RetryError[<Future at 0x7b5baa882290 state=finished raised RateLimitError>]\n",
            "!!! Error processing Network Simulation Models.pdf: RetryError[<Future at 0x7b5bacb6b210 state=finished raised RateLimitError>]\n",
            "!!! Error processing EmergencyResponseAI.pdf: RetryError[<Future at 0x7b5bad37d6d0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 11 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 58 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 71 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 73 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 75 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 88 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 96 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 98 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 141 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 143 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 150 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 156 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 191 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 200 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 206 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 249 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 261 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 271 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 323 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 325 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 345 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 352 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 397 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 400 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 402 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 416 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 444 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 473 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 475 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 506 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 512 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 522 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 710 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 712 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 718 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 724 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 730 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 900 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 902 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 904 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 942 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 954 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1094 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1146 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1148 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1150 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1156 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1179 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1271 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1273 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1292 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1302 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1321 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1356 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1358 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1368 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1370 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1425 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1427 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1429 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1452 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1458 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1460 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1462 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1497 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1538 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1544 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1575 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1577 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1592 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1598 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1625 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1627 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1629 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1639 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1645 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing LLM_UQ.pdf: RetryError[<Future at 0x7b5bad304490 state=finished raised RateLimitError>]\n",
            "!!! Error processing NAP Behavioral Sci Intel.pdf: RetryError[<Future at 0x7b5bad3f8250 state=finished raised RateLimitError>]\n",
            "!!! Error processing IkeNet.pdf: RetryError[<Future at 0x7b5bad388510 state=finished raised RateLimitError>]\n",
            "!!! Error processing improving-decision-support-for-organ-transplant.pdf: RetryError[<Future at 0x7b5ba654b010 state=finished raised RateLimitError>]\n",
            "!!! Error processing Planning for AI Sustainment A Methodology for Maintenance and Cost Management_V5.pdf: RetryError[<Future at 0x7b5bac65a3d0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing docnet.pdf: RetryError[<Future at 0x7b5bac1f2a10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Simmelian-Gamma-LDA.pdf: RetryError[<Future at 0x7b5bad354b50 state=finished raised RateLimitError>]\n",
            "!!! Error processing SocNetAlQaeda.pdf: RetryError[<Future at 0x7b5bad651610 state=finished raised RateLimitError>]\n",
            "!!! Error processing Social_Network_Probability_Mechanics.pdf: RetryError[<Future at 0x7b5bad3105d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing RatingsVRankings.pdf: RetryError[<Future at 0x7b5bad305810 state=finished raised RateLimitError>]\n",
            "!!! Error processing MLTEing_Models_for_NIER_at_ICSE_2023.pdf: RetryError[<Future at 0x7b5bac0afa10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Supply Chain Excellence.pdf: RetryError[<Future at 0x7b5bad31d3d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing ALL18.pdf: RetryError[<Future at 0x7b5bad31e790 state=finished raised RateLimitError>]\n",
            "!!! Error processing Vulnerable_Code_Detection.pdf: RetryError[<Future at 0x7b5bad31fad0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Unobtrusive Email.pdf: RetryError[<Future at 0x7b5ba6549d50 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Lead-Azide.pdf: RetryError[<Future at 0x7b5bac007d50 state=finished raised RateLimitError>]\n",
            "!!! Error processing ONA-using-igraph.pdf: RetryError[<Future at 0x7b5bac65a990 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 11 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 34 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 47 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing LLM_Confidence_Metrics.pdf: RetryError[<Future at 0x7b5bacd5a6d0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing FBI_Recruit_Hire_Final.pdf: RetryError[<Future at 0x7b5bad9abf50 state=finished raised RateLimitError>]\n",
            "!!! Error processing SecurityPrivAIML.pdf: RetryError[<Future at 0x7b5bad3e1790 state=finished raised RateLimitError>]\n",
            "!!! Error processing CUSUM Parameterization.pdf: RetryError[<Future at 0x7b5bac197710 state=finished raised RateLimitError>]\n",
            "!!! Error processing Evolution_of_Terrorism_PNAS.pdf: RetryError[<Future at 0x7b5bad6da310 state=finished raised RateLimitError>]\n",
            "!!! Error processing Course Info Security.pdf: RetryError[<Future at 0x7b5bad305e10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Analysis_of_Malware_Communities_Using_Multi_Modal_Features.pdf: RetryError[<Future at 0x7b5baaa9a6d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing A_Complex_Network_Approach_to_Find_Latent_Terorrist_Communities.pdf: RetryError[<Future at 0x7b5bac3caa10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Arrow White Paper DExTra.pdf: RetryError[<Future at 0x7b5bac3eba10 state=finished raised RateLimitError>]\n",
            "!!! Error processing Data_Education__Emerging_Challenges_and_Opportunities.pdf: RetryError[<Future at 0x7b5bac1f31d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Reforming Sectarian Beliefs.pdf: RetryError[<Future at 0x7b5bad329390 state=finished raised RateLimitError>]\n",
            "!!! Error processing Vol33Iss1_INSNApdf.pdf: RetryError[<Future at 0x7b5bacecc150 state=finished raised RateLimitError>]\n",
            "!!! Error processing Savas.pdf: RetryError[<Future at 0x7b5bacb79e90 state=finished raised RateLimitError>]\n",
            "!!! Error processing ICWSM___Use_of_Large_Language_Models_for_Stance_Classification.pdf: RetryError[<Future at 0x7b5bac147690 state=finished raised RateLimitError>]\n",
            "!!! Error processing YouTube-COVID.pdf: RetryError[<Future at 0x7b5bac3ed050 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Genetic_Algorithms_for_Prompt_Optimization.pdf: RetryError[<Future at 0x7b5bad353350 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 83 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing FSS-19_paper_137.pdf: RetryError[<Future at 0x7b5bac0466d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing ClassifiersCrowdSource.pdf: RetryError[<Future at 0x7b5bace9ab50 state=finished raised RateLimitError>]\n",
            "!!! Error processing 2024_ICWSM_Data_Challenge__Post_API_Data_Collection.pdf: RetryError[<Future at 0x7b5bad335550 state=finished raised RateLimitError>]\n",
            "!!! Error processing Misinformation_Simulation.pdf: RetryError[<Future at 0x7b5bac1586d0 state=finished raised RateLimitError>]\n",
            "!!! Error processing Organizational risk using network analysis.pdf: RetryError[<Future at 0x7b5bac65aad0 state=finished raised RateLimitError>]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!!! Error processing Overcoming_Social_Media_API_Restrictions__Building_an_Effective_Web_Scraper.pdf: RetryError[<Future at 0x7b5ba7805dd0 state=finished raised RateLimitError>]\n",
            "==================================================\n",
            "Department of Health and Human Services\n",
            "Part 1. Overview Information\n",
            "Key Dates\n",
            "The following table includes NIH standard due dates (https://grants.nih.gov/grants/how-to-apply-application-guide/due-dates-and-submission-policies/due-dates.htm) marked with anasterisk.\n",
            "Application Due Dates Review and Award Cycles\n",
            "New\n",
            "Renewal /Resubmission /Revision (asallowed)\n",
            "AIDS -New/Renewal/Resubmission/Revision,as allowed Scientiﬁc MeritReview Advisory CouncilReviewEarliest Start Date\n",
            "February 05, 2025 * March\n",
            "Relevant Papers: 0/112\n",
            "Irrelevant Papers: 112/112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "file_list = [\n",
        "    \"/content/research_papers/brain_ai_2023.pdf\",\n",
        "    \"/content/research_papers/equity_neuro_biomarkers.pdf\"\n",
        "]\n",
        "\n",
        "response = pipeline.invoke(file_list)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "tUFew5lBjkUr",
        "outputId": "227dbaa3-0324-4452-94a3-05a9bf0d0384"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nofo_summary_text' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-5-1483691674.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m ]\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3772\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3773\u001b[0m                 ]\n\u001b[0;32m-> 3774\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3775\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3772\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3773\u001b[0m                 ]\n\u001b[0;32m-> 3774\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3775\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    447\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    448\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 449\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3756\u001b[0m             )\n\u001b[1;32m   3757\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3758\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3759\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                     \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[1;32m   4771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4772\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   4773\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4774\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m                 output = cast(\n\u001b[1;32m   1939\u001b[0m                     \u001b[0;34m\"Output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                     context.run(\n\u001b[0m\u001b[1;32m   1941\u001b[0m                         \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input_, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4628\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4630\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   4631\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-5-1483691674.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(_)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;31m# Static NOFO summary (can also be dynamic)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0mnofo_summary_runnable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunnableLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"nofo_summary\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnofo_summary_text\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;31m# Compose the pipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nofo_summary_text' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "file_list = [\n",
        "    \"/content/research_papers/brain_ai_2023.pdf\",\n",
        "    \"/content/research_papers/equity_neuro_biomarkers.pdf\"\n",
        "]\n",
        "\n",
        "response = pipeline.invoke(file_list)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "HaVCPEhwkuNj",
        "outputId": "27928a18-ab25-4eb9-e191-ceacdb24fa25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "File path /content/research_papers/brain_ai_2023.pdf is not a valid file or url",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-6-1566365960.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     86\u001b[0m ]\n\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3043\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3044\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3045\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3046\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3047\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3772\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3773\u001b[0m                 ]\n\u001b[0;32m-> 3774\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3775\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   3772\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3773\u001b[0m                 ]\n\u001b[0;32m-> 3774\u001b[0;31m                 \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfuture\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3775\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3776\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    454\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m                 \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    399\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    400\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 401\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    402\u001b[0m             \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    403\u001b[0m                 \u001b[0;31m# Break a reference cycle with the exception in self._exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke_step\u001b[0;34m(step, input_, config, key)\u001b[0m\n\u001b[1;32m   3756\u001b[0m             )\n\u001b[1;32m   3757\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mset_config_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchild_config\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3758\u001b[0;31m                 return context.run(\n\u001b[0m\u001b[1;32m   3759\u001b[0m                     \u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3760\u001b[0m                     \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   4770\u001b[0m         \"\"\"\n\u001b[1;32m   4771\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"func\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4772\u001b[0;31m             return self._call_with_config(\n\u001b[0m\u001b[1;32m   4773\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4774\u001b[0m                 \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input_, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1938\u001b[0m                 output = cast(\n\u001b[1;32m   1939\u001b[0m                     \u001b[0;34m\"Output\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1940\u001b[0;31m                     context.run(\n\u001b[0m\u001b[1;32m   1941\u001b[0m                         \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m                         \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input_, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m   4628\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4629\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4630\u001b[0;31m             output = call_func_with_variable_args(\n\u001b[0m\u001b[1;32m   4631\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4632\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-1566365960.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(file_paths)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m# Lambda wrapper for integration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mdoc_loader_runnable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRunnableLambda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"research_chunks\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mload_and_combine_docs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Define nofo_summary_text using the previously loaded NOFO_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-6-1566365960.py\u001b[0m in \u001b[0;36mload_and_combine_docs\u001b[0;34m(file_paths)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mdocs_with_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfile_paths\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mloader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0mdocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m         \u001b[0mfull_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_content\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, password, headers, extract_images, mode, images_parser, images_inner_format, pages_delimiter, extraction_mode, extraction_kwargs)\u001b[0m\n\u001b[1;32m    279\u001b[0m             \u001b[0;31m`\u001b[0m\u001b[0maload\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mmethods\u001b[0m \u001b[0mto\u001b[0m \u001b[0mretrieve\u001b[0m \u001b[0mparsed\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mcontent\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m         \"\"\"\n\u001b[0;32m--> 281\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    282\u001b[0m         self.parser = PyPDFParser(\n\u001b[1;32m    283\u001b[0m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpassword\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/document_loaders/pdf.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file_path, headers)\u001b[0m\n\u001b[1;32m    138\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_pdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File path %s is not a valid file or url\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__del__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: File path /content/research_papers/brain_ai_2023.pdf is not a valid file or url"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "response = pipeline.invoke(file_list)\n",
        "print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cZuYLJkym8ay",
        "outputId": "2bc4b811-26ee-40ab-ca11-c21fe9ada5ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 28 0 (offset 0)\n",
            "WARNING:pypdf.generic._data_structures:Multiple definitions in dictionary at byte 0x8fa1a for key /PageMode\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 9 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 51 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 54 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 56 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 40 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 28 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 25 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 35 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 37 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 45 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 53 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 55 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 57 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 59 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 61 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 68 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 15 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 19 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 30 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 32 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 44 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 16 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 28 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 15 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 18 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 48 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 92 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 6 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 36 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 40 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 42 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 52 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 54 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 50 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 21 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 23 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 29 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 43 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 52 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 11 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 58 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 69 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 71 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 73 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 75 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 88 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 90 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 96 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 98 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 141 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 143 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 150 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 156 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 191 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 193 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 200 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 206 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 249 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 251 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 261 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 271 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 323 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 325 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 345 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 352 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 397 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 400 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 402 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 416 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 444 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 473 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 475 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 506 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 512 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 522 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 710 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 712 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 718 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 724 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 730 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 900 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 902 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 904 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 942 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 954 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1094 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1146 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1148 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1150 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1156 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1179 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1271 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1273 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1292 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1302 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1321 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1356 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1358 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1368 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1370 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1425 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1427 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1429 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1452 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1458 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1460 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1462 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1497 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1538 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1544 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1575 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1577 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1592 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1598 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1625 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1627 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1629 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1639 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 1645 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 38 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 12 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 20 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 26 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 39 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 41 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 11 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 14 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 34 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 47 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 8 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 10 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 13 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 22 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 24 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 27 0 (offset 0)\n",
            "WARNING:pypdf._reader:Ignoring wrong pointing object 83 0 (offset 0)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RateLimitError",
          "evalue": "Error code: 429 - {'reason': {'error': 'You exceeded your current quota!!'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-10-3642802393.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;31m# ]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3045\u001b[0m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3046\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3047\u001b[0;31m                         \u001b[0minput_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3048\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3049\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_chain_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m             outputs = (\n\u001b[0;32m--> 160\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mnew_arg_supported\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0mrun_manager\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mCallbackManagerForChainRun\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m     ) -> Dict[str, str]:\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_outputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain/chains/llm.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mcallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_child\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBaseLanguageModel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m             return self.llm.generate_prompt(\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0mprompts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                 \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    955\u001b[0m     ) -> LLMResult:\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverride\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    774\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m                         \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1020\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_from_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         }\n\u001b[0;32m--> 476\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1006\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mremaining_retries\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_retry\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1007\u001b[0m                 \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1008\u001b[0;31m                 return self._retry_request(\n\u001b[0m\u001b[1;32m   1009\u001b[0m                     \u001b[0minput_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1010\u001b[0m                     \u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_retry_request\u001b[0;34m(self, options, cast_to, retries_taken, response_headers, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1055\u001b[0m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1057\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m   1058\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1059\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'reason': {'error': 'You exceeded your current quota!!'}}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\ttry:\n",
        "            loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "Yu5cs_ojtCPB",
        "outputId": "8a244304-db85-4f0f-8b51-4045a2d852c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    loader = PyPDFLoader(path)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\ttry:\n",
        "            loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "P3BgDm3Wu-Fp",
        "outputId": "071c5960-7f78-4131-be99-88210b9571dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    loader = PyPDFLoader(path)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\ttry:\n",
        "              loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "5VgaQED-wxVC",
        "outputId": "3de68a03-2ac5-4189-9ac8-52b6f93d652e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    loader = PyPDFLoader(path)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\ttry:\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        # except Exception as e:\n",
        "            # print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            # docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "5KCoiG7pxtNo",
        "outputId": "32f1daad-78b7-4563-9824-0af3db46a5bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    loader = PyPDFLoader(path)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\ttry:\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        # except Exception as e:\n",
        "            # print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            # docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "Q3aXw8GSx3sZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\ttry:\n",
        "        loader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        # except Exception as e:\n",
        "            # print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            # docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "id": "XKAW2MFZyAGZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\tloader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        # except Exception as e:\n",
        "            # print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            # docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "HnMTl-uuyRcA",
        "outputId": "231ad186-fa00-4cba-c0b5-c2c314affd33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    docs = loader.load()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "from openai import RateLimitError # Import RateLimitError for retry logic\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "\t\tloader = PyPDFLoader(path)\n",
        "        docs = loader.load()\n",
        "        full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "        docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\") # Increased chunk size for potential benefit\n",
        "        # except Exception as e:\n",
        "            # print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            # docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "YPEnLD0hyizh",
        "outputId": "6121d708-469a-4149-b3cf-16a1bdbd03e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndentationError",
          "evalue": "unindent does not match any outer indentation level (<tokenize>, line 56)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m56\u001b[0m\n\u001b[0;31m    docs = loader.load()\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "# Correct the import path for ChatOpenAI\n",
        "\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "# Correct the import path for PyPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "# Ensure openai is imported correctly if RateLimitError is used\n",
        "try:\n",
        "    from openai import RateLimitError\n",
        "except ImportError:\n",
        "    # Handle cases where RateLimitError might be in a different location or version\n",
        "    print(\"Warning: openai.RateLimitError not found. Retry might not handle rate limits correctly.\")\n",
        "    # Define a dummy exception class if not found, or use a more general retry condition\n",
        "    class RateLimitError(Exception):\n",
        "        pass\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "!pip install langchain-community\n",
        "import os\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "        try:\n",
        "            loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            # Increased chunk size for potential benefit\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 385
        },
        "id": "Udkgga9nzYwR",
        "outputId": "88b1d34c-fc0f-4f8d-9507-b6902dc488ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'langchain_community'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3-4265644730.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchains\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Correct the import path for ChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_models\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m# Correct the import path for PyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPyPDFLoader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "# Correct the import path for ChatOpenAI\n",
        "!pip install langchain-community\n",
        "from langchain_community.chat_models import ChatOpenAI\n",
        "# Correct the import path for PyPDFLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "import os\n",
        "\n",
        "import tenacity\n",
        "import logging\n",
        "# Ensure openai is imported correctly if RateLimitError is used\n",
        "try:\n",
        "    from openai import RateLimitError\n",
        "except ImportError:\n",
        "    # Handle cases where RateLimitError might be in a different location or version\n",
        "    print(\"Warning: openai.RateLimitError not found. Retry might not handle rate limits correctly.\")\n",
        "    # Define a dummy exception class if not found, or use a more general retry condition\n",
        "    class RateLimitError(Exception):\n",
        "        pass\n",
        "\n",
        "import tiktoken\n",
        "\n",
        "# Ensure logging is configured for tenacity\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "\n",
        "proposal_prompt = PromptTemplate(\n",
        "    input_variables=[\"nofo_summary\", \"research_chunks\"],\n",
        "    template=\"\"\"\n",
        "You are a research strategy assistant tasked with generating 5 actionable and innovative research proposal ideas based on the content of scientific research papers and the requirements outlined in the NOFO.\n",
        "\n",
        "Using the context provided below, produce exactly **5 distinct project ideas** in the following structured format:\n",
        "\n",
        "---\n",
        "1. **Idea {{i}}:** [Concise Title of the Project Idea]\n",
        "2. **Description:** [Brief and targeted description summarizing the objectives, innovative elements, scientific rationale, and anticipated impact.]\n",
        "3. **Citation:** [Author(s), Year or Paper Title]\n",
        "4. **NOFO Alignment:** [List two or more specific NOFO requirements that this idea directly addresses]\n",
        "5. **File Path of the Research Paper:** [Exact file path, ending in .pdf]\n",
        "---\n",
        "\n",
        "**NOFO Summary:**\n",
        "{nofo_summary}\n",
        "\n",
        "**Research Paper Chunks (with file paths):**\n",
        "{research_chunks}\n",
        "\n",
        "Respond in plain text. Be specific, and make sure file paths and citations align.\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "\n",
        "# LLM setup\n",
        "llm = ChatOpenAI(temperature=0.5)\n",
        "llm_chain = LLMChain(llm=llm, prompt=proposal_prompt)\n",
        "\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "# Load and parse PDF files into text chunks + file path\n",
        "def load_and_combine_docs(file_paths: list):\n",
        "    docs_with_paths = []\n",
        "    for path in file_paths:\n",
        "        try:\n",
        "            loader = PyPDFLoader(path)\n",
        "            docs = loader.load()\n",
        "            full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "            # Increased chunk size for potential benefit\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:2000]}...\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading or processing {path}: {e}\")\n",
        "            # Optionally skip this file or add a placeholder\n",
        "            docs_with_paths.append(f\"Paper: {path}\\nContent: Error loading file.\")\n",
        "    return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "    #     loader = PyPDFLoader(path)\n",
        "    #   docs = loader.load()\n",
        "    #   full_text = \"\\n\".join([doc.page_content for doc in docs])\n",
        "    #   docs_with_paths.append(f\"Paper: {path}\\nContent: {full_text[:1000]}...\")  # limit to 1000 chars per paper\n",
        "    # return \"\\n\\n\".join(docs_with_paths)\n",
        "\n",
        "# Lambda wrapper for integration\n",
        "doc_loader_runnable = RunnableLambda(lambda file_paths: {\"research_chunks\": load_and_combine_docs(file_paths)})\n",
        "\n",
        "# Define nofo_summary_text using the previously loaded NOFO_text\n",
        "# Make sure NOFO_text is accessible in this cell or pass it appropriately\n",
        "# For simplicity, assuming NOFO_text is available from a previous cell's execution\n",
        "# If not, you would need to load or pass it here.\n",
        "try:\n",
        "    # Attempt to use the NOFO_text from the previous cell\n",
        "    # If running as separate cells, this variable must be defined in a preceding cell\n",
        "    nofo_summary_text = NOFO_text\n",
        "except NameError:\n",
        "    # Handle the case where NOFO_text is not defined (e.g., if cells are run out of order)\n",
        "    print(\"Warning: NOFO_text not found. Loading NOFO again for summary.\")\n",
        "    # Reload NOFO for use in prompt if NOFO_text is not available\n",
        "    nofo_loader = PyPDFLoader(\"/content/NOFO_pdf.pdf\")\n",
        "    nofo_docs = nofo_loader.load()\n",
        "    nofo_summary_text = \"\\n\\n\".join([doc.page_content for doc in nofo_docs])\n",
        "    nofo_summary_text = nofo_summary_text[:12000] # Apply truncation again\n",
        "\n",
        "\n",
        "# Static NOFO summary (can also be dynamic)\n",
        "nofo_summary_runnable = RunnableLambda(lambda _: {\"nofo_summary\": nofo_summary_text})\n",
        "\n",
        "# Compose the pipeline\n",
        "pipeline = RunnableParallel({\n",
        "    \"nofo_summary\": nofo_summary_runnable,\n",
        "    \"research_chunks\": doc_loader_runnable\n",
        "}) | llm_chain\n",
        "\n",
        "# Dynamically build the file_list from the correct directory\n",
        "# Ensure papers_path is accessible (it should be from the preceding cells)\n",
        "try:\n",
        "    papers_path\n",
        "except NameError:\n",
        "    print(\"Error: papers_path is not defined. Please ensure the previous cells were run.\")\n",
        "    # You might need to define papers_path here if the previous cell wasn't run.\n",
        "    # For example: papers_path = \"/content/Papers/\" # Or the correct path\n",
        "\n",
        "# Build the list of PDF files from the correct directory\n",
        "file_list = [os.path.join(papers_path, f) for f in os.listdir(papers_path) if f.endswith(\".pdf\")]\n",
        "\n",
        "# Implement retry mechanism for the main pipeline execution\n",
        "@tenacity.retry(\n",
        "    wait=tenacity.wait_exponential(multiplier=1, min=4, max=60), # Wait exponentially between retries\n",
        "    stop=tenacity.stop_after_attempt(5), # Stop after 5 attempts\n",
        "    before_sleep=tenacity.before_sleep_log(logging.getLogger(__name__), logging.INFO), # Log before sleeping\n",
        "    retry=tenacity.retry_if_exception_type(RateLimitError) # Retry specifically on RateLimitError\n",
        ")\n",
        "def invoke_pipeline_with_retry(file_list):\n",
        "    \"\"\"Helper function to invoke the pipeline with retry logic.\"\"\"\n",
        "    return pipeline.invoke(file_list)\n",
        "\n",
        "# Call the pipeline using the retry function\n",
        "try:\n",
        "    response = invoke_pipeline_with_retry(file_list)\n",
        "    print(response)\n",
        "except RateLimitError as e:\n",
        "    print(f\"Failed to generate proposal ideas after multiple retries due to rate limit: {e}\")\n",
        "except Exception as e:\n",
        "    print(f\"An unexpected error occurred during pipeline execution: {e}\")\n",
        "\n",
        "\n",
        "# file_list = [\n",
        "#   \"/content/papers/brain_ai_2023.pdf\",\n",
        "#   \"/content/papers/equity_neuro_biomarkers.pdf\"\n",
        "# ]\n",
        "\n",
        "# response = pipeline.invoke(file_list)\n",
        "# print(response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8a9ca3cb-06d8-431b-d81c-aa9a3096d44e",
        "id": "H8noMjF_yJIk"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.66)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.1.2)\n",
            "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n",
            "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.0)\n",
            "Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.45)\n",
            "Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n",
            "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n",
            "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-community) (4.14.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n",
            "Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.3.1)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValidationError",
          "evalue": "1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0.5, 'mod...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2-3931215295.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# LLM setup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0mllm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mChatOpenAI\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0mllm_chain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLLMChain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mproposal_prompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/_api/deprecation.py\u001b[0m in \u001b[0;36mwarn_if_direct_instance\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    220\u001b[0m                         \u001b[0mwarned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m                         \u001b[0memit_warning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m                 obj.__init__ = functools.wraps(obj.__init__)(  # type: ignore[misc]\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/load/serializable.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;34m\"\"\"\"\"\"\u001b[0m  \u001b[0;31m# noqa: D419\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pydantic/main.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, **data)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;31m# `__tracebackhide__` tells pytest and some other tools to omit this function from tracebacks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0m__tracebackhide__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m         \u001b[0mvalidated_self\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__pydantic_validator__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself_instance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mvalidated_self\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             warnings.warn(\n",
            "\u001b[0;31mValidationError\u001b[0m: 1 validation error for ChatOpenAI\n  Value error, Did not find openai_api_key, please add an environment variable `OPENAI_API_KEY` which contains it, or pass `openai_api_key` as a named parameter. [type=value_error, input_value={'temperature': 0.5, 'mod...ne, 'http_client': None}, input_type=dict]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
          ]
        }
      ]
    }
  ]
}